{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff864f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "F = 0\n",
    "B = 1\n",
    "R = 2\n",
    "L = 3\n",
    "U = 4\n",
    "D = 5\n",
    "\n",
    "\n",
    "class Cube2(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = gym.spaces.Discrete(12)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1, shape=(24, 6), dtype=np.int8\n",
    "        )\n",
    "        self.state = np.zeros((6, 4))\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.state = np.zeros((6, 4))\n",
    "        self.state[0] = np.ones(4) * F\n",
    "        self.state[1] = np.ones(4) * B\n",
    "        self.state[2] = np.ones(4) * R\n",
    "        self.state[3] = np.ones(4) * L\n",
    "        self.state[4] = np.ones(4) * U\n",
    "        self.state[5] = np.ones(4) * D\n",
    "        self.step_count = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        new_state = self.state.copy()\n",
    "\n",
    "        # Front Clockwise\n",
    "        if action == 0:\n",
    "            new_state[F, 0] = self.state[F, 2]\n",
    "            new_state[F, 1] = self.state[F, 0]\n",
    "            new_state[F, 2] = self.state[F, 3]\n",
    "            new_state[F, 3] = self.state[F, 1]\n",
    "            new_state[R, 1] = self.state[U, 3]\n",
    "            new_state[R, 3] = self.state[U, 1]\n",
    "            new_state[L, 1] = self.state[D, 3]\n",
    "            new_state[L, 3] = self.state[D, 1]\n",
    "            new_state[U, 1] = self.state[L, 1]\n",
    "            new_state[U, 3] = self.state[L, 3]\n",
    "            new_state[D, 1] = self.state[R, 1]\n",
    "            new_state[D, 3] = self.state[R, 3]\n",
    "        # Front Counter-Clockwise\n",
    "        elif action == 1:\n",
    "            new_state[F, 0] = self.state[F, 1]\n",
    "            new_state[F, 1] = self.state[F, 3]\n",
    "            new_state[F, 2] = self.state[F, 0]\n",
    "            new_state[F, 3] = self.state[F, 2]\n",
    "            new_state[R, 1] = self.state[D, 1]\n",
    "            new_state[R, 3] = self.state[D, 3]\n",
    "            new_state[L, 1] = self.state[U, 1]\n",
    "            new_state[L, 3] = self.state[U, 3]\n",
    "            new_state[U, 1] = self.state[R, 3]\n",
    "            new_state[U, 3] = self.state[R, 1]\n",
    "            new_state[D, 1] = self.state[L, 3]\n",
    "            new_state[D, 3] = self.state[L, 1]\n",
    "        # Back Clockwise\n",
    "        elif action == 2:\n",
    "            new_state[B, 0] = self.state[B, 1]\n",
    "            new_state[B, 1] = self.state[B, 3]\n",
    "            new_state[B, 2] = self.state[B, 0]\n",
    "            new_state[B, 3] = self.state[B, 2]\n",
    "            new_state[R, 0] = self.state[D, 0]\n",
    "            new_state[R, 2] = self.state[D, 2]\n",
    "            new_state[L, 0] = self.state[U, 0]\n",
    "            new_state[L, 2] = self.state[U, 2]\n",
    "            new_state[U, 0] = self.state[R, 2]\n",
    "            new_state[U, 2] = self.state[R, 0]\n",
    "            new_state[D, 0] = self.state[L, 2]\n",
    "            new_state[D, 2] = self.state[L, 0]\n",
    "        # Back Counter-Clockwise\n",
    "        elif action == 3:\n",
    "            new_state[B, 0] = self.state[B, 2]\n",
    "            new_state[B, 1] = self.state[B, 0]\n",
    "            new_state[B, 2] = self.state[B, 3]\n",
    "            new_state[B, 3] = self.state[B, 1]\n",
    "            new_state[R, 0] = self.state[U, 2]\n",
    "            new_state[R, 2] = self.state[U, 0]\n",
    "            new_state[L, 0] = self.state[D, 2]\n",
    "            new_state[L, 2] = self.state[D, 0]\n",
    "            new_state[U, 0] = self.state[L, 0]\n",
    "            new_state[U, 2] = self.state[L, 2]\n",
    "            new_state[D, 0] = self.state[R, 0]\n",
    "            new_state[D, 2] = self.state[R, 2]\n",
    "        # Right Clockwise\n",
    "        elif action == 4:\n",
    "            new_state[F, 2] = self.state[D, 2]\n",
    "            new_state[F, 3] = self.state[D, 3]\n",
    "            new_state[B, 2] = self.state[U, 2]\n",
    "            new_state[B, 3] = self.state[U, 3]\n",
    "            new_state[R, 0] = self.state[R, 2]\n",
    "            new_state[R, 1] = self.state[R, 0]\n",
    "            new_state[R, 2] = self.state[R, 3]\n",
    "            new_state[R, 3] = self.state[R, 1]\n",
    "            new_state[U, 2] = self.state[F, 3]\n",
    "            new_state[U, 3] = self.state[F, 2]\n",
    "            new_state[D, 2] = self.state[B, 3]\n",
    "            new_state[D, 3] = self.state[B, 2]\n",
    "        # Right Counter-Clockwise\n",
    "        elif action == 5:\n",
    "            new_state[F, 2] = self.state[U, 3]\n",
    "            new_state[F, 3] = self.state[U, 2]\n",
    "            new_state[B, 2] = self.state[D, 3]\n",
    "            new_state[B, 3] = self.state[D, 2]\n",
    "            new_state[R, 0] = self.state[R, 1]\n",
    "            new_state[R, 1] = self.state[R, 3]\n",
    "            new_state[R, 2] = self.state[R, 0]\n",
    "            new_state[R, 3] = self.state[R, 2]\n",
    "            new_state[U, 2] = self.state[B, 2]\n",
    "            new_state[U, 3] = self.state[B, 3]\n",
    "            new_state[D, 2] = self.state[F, 2]\n",
    "            new_state[D, 3] = self.state[F, 3]\n",
    "        # Left Clockwise\n",
    "        elif action == 6:\n",
    "            new_state[F, 0] = self.state[U, 1]\n",
    "            new_state[F, 1] = self.state[U, 0]\n",
    "            new_state[B, 0] = self.state[D, 1]\n",
    "            new_state[B, 1] = self.state[D, 0]\n",
    "            new_state[L, 0] = self.state[L, 1]\n",
    "            new_state[L, 1] = self.state[L, 3]\n",
    "            new_state[L, 2] = self.state[L, 0]\n",
    "            new_state[L, 3] = self.state[L, 2]\n",
    "            new_state[U, 0] = self.state[B, 0]\n",
    "            new_state[U, 1] = self.state[B, 1]\n",
    "            new_state[D, 0] = self.state[F, 0]\n",
    "            new_state[D, 1] = self.state[F, 1]\n",
    "        # Left Counter-Clockwise\n",
    "        elif action == 7:\n",
    "            new_state[F, 0] = self.state[D, 0]\n",
    "            new_state[F, 1] = self.state[D, 1]\n",
    "            new_state[B, 0] = self.state[U, 0]\n",
    "            new_state[B, 1] = self.state[U, 1]\n",
    "            new_state[L, 0] = self.state[L, 2]\n",
    "            new_state[L, 1] = self.state[L, 0]\n",
    "            new_state[L, 2] = self.state[L, 3]\n",
    "            new_state[L, 3] = self.state[L, 1]\n",
    "            new_state[U, 0] = self.state[F, 1]\n",
    "            new_state[U, 1] = self.state[F, 0]\n",
    "            new_state[D, 0] = self.state[B, 1]\n",
    "            new_state[D, 1] = self.state[B, 0]\n",
    "        # Up Clockwise\n",
    "        elif action == 8:\n",
    "            new_state[F, 1] = self.state[R, 3]\n",
    "            new_state[F, 3] = self.state[R, 2]\n",
    "            new_state[B, 1] = self.state[L, 3]\n",
    "            new_state[B, 3] = self.state[L, 2]\n",
    "            new_state[R, 2] = self.state[B, 1]\n",
    "            new_state[R, 3] = self.state[B, 3]\n",
    "            new_state[L, 2] = self.state[F, 1]\n",
    "            new_state[L, 3] = self.state[F, 3]\n",
    "            new_state[U, 0] = self.state[U, 1]\n",
    "            new_state[U, 1] = self.state[U, 3]\n",
    "            new_state[U, 2] = self.state[U, 0]\n",
    "            new_state[U, 3] = self.state[U, 2]\n",
    "        # Up Counter-Clockwise\n",
    "        elif action == 9:\n",
    "            new_state[F, 1] = self.state[L, 2]\n",
    "            new_state[F, 3] = self.state[L, 3]\n",
    "            new_state[B, 1] = self.state[R, 2]\n",
    "            new_state[B, 3] = self.state[R, 3]\n",
    "            new_state[R, 2] = self.state[F, 3]\n",
    "            new_state[R, 3] = self.state[F, 1]\n",
    "            new_state[L, 2] = self.state[B, 3]\n",
    "            new_state[L, 3] = self.state[B, 1]\n",
    "            new_state[U, 0] = self.state[U, 2]\n",
    "            new_state[U, 1] = self.state[U, 0]\n",
    "            new_state[U, 2] = self.state[U, 3]\n",
    "            new_state[U, 3] = self.state[U, 1]\n",
    "        # Bottom Clockwise\n",
    "        elif action == 10:\n",
    "            new_state[F, 0] = self.state[L, 0]\n",
    "            new_state[F, 2] = self.state[L, 1]\n",
    "            new_state[B, 0] = self.state[R, 0]\n",
    "            new_state[B, 2] = self.state[R, 1]\n",
    "            new_state[R, 0] = self.state[F, 2]\n",
    "            new_state[R, 1] = self.state[F, 0]\n",
    "            new_state[L, 0] = self.state[B, 2]\n",
    "            new_state[L, 1] = self.state[B, 0]\n",
    "            new_state[D, 0] = self.state[D, 2]\n",
    "            new_state[D, 1] = self.state[D, 0]\n",
    "            new_state[D, 2] = self.state[D, 3]\n",
    "            new_state[D, 3] = self.state[D, 1]\n",
    "        # Bottom Counter-Clockwise\n",
    "        elif action == 11:\n",
    "            new_state[F, 0] = self.state[R, 1]\n",
    "            new_state[F, 2] = self.state[R, 0]\n",
    "            new_state[B, 0] = self.state[L, 1]\n",
    "            new_state[B, 2] = self.state[L, 0]\n",
    "            new_state[R, 0] = self.state[B, 0]\n",
    "            new_state[R, 1] = self.state[B, 2]\n",
    "            new_state[L, 0] = self.state[F, 0]\n",
    "            new_state[L, 1] = self.state[F, 2]\n",
    "            new_state[D, 0] = self.state[D, 1]\n",
    "            new_state[D, 1] = self.state[D, 3]\n",
    "            new_state[D, 2] = self.state[D, 0]\n",
    "            new_state[D, 3] = self.state[D, 2]\n",
    "        self.state = new_state\n",
    "        return (\n",
    "            self._get_obs(),\n",
    "            1 if self._is_solved() else -1,\n",
    "            self._is_solved(),\n",
    "            self.step_count >= 100,\n",
    "            {},\n",
    "        )\n",
    "\n",
    "    def _get_obs(self):\n",
    "        one_hots = []\n",
    "        for i in range(6):\n",
    "            for j in range(4):\n",
    "                label = int(self.state[i, j])\n",
    "                zeros = np.zeros(6)\n",
    "                zeros[label] = 1\n",
    "                one_hots.append(zeros)\n",
    "        return np.array(one_hots)\n",
    "\n",
    "    def _is_solved(self):\n",
    "        for i in range(6):\n",
    "            if np.mean(self.state[i]) != self.state[i][0]:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624c83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def state(self):\n",
    "        return self.env.state\n",
    "\n",
    "    def step_count(self):\n",
    "        return self.env.step_count\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.env.reset(*args, **kwargs)\n",
    "        for _ in range(4):\n",
    "            self.env.step(self.env.action_space.sample())\n",
    "        self.env.step_count = 0\n",
    "        return self.env._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, _ = super().step(action)\n",
    "        if terminated:\n",
    "            reward = 100\n",
    "        return obs, reward, terminated, truncated, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = RewardWrapper(Cube2())\n",
    "obs, _ = env.reset()\n",
    "print(env.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.2     |\n",
      "|    ep_rew_mean      | -88.2    |\n",
      "|    exploration_rate | 0.105    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 4943     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 9424     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0004   |\n",
      "|    n_updates        | 2330     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.1     |\n",
      "|    ep_rew_mean      | -96.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 4426     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 19236    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000292 |\n",
      "|    n_updates        | 4783     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.2     |\n",
      "|    ep_rew_mean      | -90.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 4349     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 28754    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000103 |\n",
      "|    n_updates        | 7163     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.4     |\n",
      "|    ep_rew_mean      | -76.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 4391     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 37598    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000121 |\n",
      "|    n_updates        | 9374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.6     |\n",
      "|    ep_rew_mean      | -72.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 4417     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 46260    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000169 |\n",
      "|    n_updates        | 11539    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 82.6     |\n",
      "|    ep_rew_mean      | -64.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 4436     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 54520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.72e-05 |\n",
      "|    n_updates        | 13604    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 79.4     |\n",
      "|    ep_rew_mean      | -57.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 4445     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 62462    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.99e-05 |\n",
      "|    n_updates        | 15590    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 75.5     |\n",
      "|    ep_rew_mean      | -49.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 4456     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 70012    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.264    |\n",
      "|    n_updates        | 17477    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.5     |\n",
      "|    ep_rew_mean      | -39.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 4471     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 77066    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000102 |\n",
      "|    n_updates        | 19241    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.1     |\n",
      "|    ep_rew_mean      | -28.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 4489     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 83678    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000145 |\n",
      "|    n_updates        | 20894    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.9     |\n",
      "|    ep_rew_mean      | -31.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100     |\n",
      "|    fps              | 4504     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 90370    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000488 |\n",
      "|    n_updates        | 22567    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.6     |\n",
      "|    ep_rew_mean      | -34.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200     |\n",
      "|    fps              | 4517     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 97230    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00045  |\n",
      "|    n_updates        | 24282    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = Cube2()\n",
    "env = RewardWrapper(env)\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=100000, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24132717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotationController.setState([[0.0, 0.0, 3.0, 4.0], [5.0, 2.0, 1.0, 1.0], [3.0, 4.0, 3.0, 2.0], [2.0, 5.0, 4.0, 5.0], [0.0, 3.0, 5.0, 1.0], [1.0, 2.0, 4.0, 0.0]])\n",
      "rotationController.addRotationStepCode(...[3, 1, 8, 3])\n",
      "\n",
      "Solved in 4 steps\n"
     ]
    }
   ],
   "source": [
    "# model = DQN.load(\"dqn_cube2.pkl\")\n",
    "import json\n",
    "\n",
    "env = Cube2()\n",
    "env = RewardWrapper(env)\n",
    "obs, _ = env.reset()\n",
    "print(f\"rotationController.setState({json.dumps(env.state().tolist())})\")\n",
    "\n",
    "solved_actions = []\n",
    "for i in range(100):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    solved_actions.append(action.item())\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "print(f\"rotationController.addRotationStepCode(...{json.dumps(solved_actions)})\")\n",
    "\n",
    "print()\n",
    "print(f\"Solved in {len(solved_actions)} steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
