{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dff864f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "F = 0\n",
    "B = 1\n",
    "R = 2\n",
    "L = 3\n",
    "U = 4\n",
    "D = 5\n",
    "\n",
    "class Cube2(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = gym.spaces.Discrete(12)\n",
    "        self.observation_space = gym.spaces.Box(low=0,high=1,shape=(24, 6),dtype=np.int8)\n",
    "        self.state = np.zeros((6, 4))\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.state = np.zeros((6, 4))\n",
    "        self.state[0] = np.ones(4) * F\n",
    "        self.state[1] = np.ones(4) * B\n",
    "        self.state[2] = np.ones(4) * R\n",
    "        self.state[3] = np.ones(4) * L \n",
    "        self.state[4] = np.ones(4) * U\n",
    "        self.state[5] = np.ones(4) * D\n",
    "        self.step_count = 0\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        new_state = self.state.copy()\n",
    "\n",
    "        # Front Clockwise\n",
    "        if action == 0:\n",
    "            new_state[F, 0] = self.state[F, 2]\n",
    "            new_state[F, 1] = self.state[F, 0]\n",
    "            new_state[F, 2] = self.state[F, 3]\n",
    "            new_state[F, 3] = self.state[F, 1]\n",
    "            new_state[R, 1] = self.state[U, 3]\n",
    "            new_state[R, 3] = self.state[U, 1]\n",
    "            new_state[L, 1] = self.state[D, 3]\n",
    "            new_state[L, 3] = self.state[D, 1]\n",
    "            new_state[U, 1] = self.state[L, 1]\n",
    "            new_state[U, 3] = self.state[L, 3]\n",
    "            new_state[D, 1] = self.state[R, 1]\n",
    "            new_state[D, 3] = self.state[R, 3]\n",
    "        # Front Counter-Clockwise\n",
    "        elif action == 1:\n",
    "            new_state[F, 0] = self.state[F, 1]\n",
    "            new_state[F, 1] = self.state[F, 3]\n",
    "            new_state[F, 2] = self.state[F, 0]\n",
    "            new_state[F, 3] = self.state[F, 2]\n",
    "            new_state[R, 1] = self.state[D, 1]\n",
    "            new_state[R, 3] = self.state[D, 3]\n",
    "            new_state[L, 1] = self.state[U, 1]\n",
    "            new_state[L, 3] = self.state[U, 3]\n",
    "            new_state[U, 1] = self.state[R, 3]\n",
    "            new_state[U, 3] = self.state[R, 1]\n",
    "            new_state[D, 1] = self.state[L, 3]\n",
    "            new_state[D, 3] = self.state[L, 1]\n",
    "        # Back Clockwise\n",
    "        elif action == 2:\n",
    "            new_state[B, 0] = self.state[B, 1]\n",
    "            new_state[B, 1] = self.state[B, 3]\n",
    "            new_state[B, 2] = self.state[B, 0]\n",
    "            new_state[B, 3] = self.state[B, 2]\n",
    "            new_state[R, 0] = self.state[D, 0]\n",
    "            new_state[R, 2] = self.state[D, 2]\n",
    "            new_state[L, 0] = self.state[U, 0]\n",
    "            new_state[L, 2] = self.state[U, 2]\n",
    "            new_state[U, 0] = self.state[R, 2]\n",
    "            new_state[U, 2] = self.state[R, 0]\n",
    "            new_state[D, 0] = self.state[L, 2]\n",
    "            new_state[D, 2] = self.state[L, 0]\n",
    "        # Back Counter-Clockwise\n",
    "        elif action == 3:\n",
    "            new_state[B, 0] = self.state[B, 2]\n",
    "            new_state[B, 1] = self.state[B, 0]\n",
    "            new_state[B, 2] = self.state[B, 3]\n",
    "            new_state[B, 3] = self.state[B, 1]\n",
    "            new_state[R, 0] = self.state[U, 2]\n",
    "            new_state[R, 2] = self.state[U, 0]\n",
    "            new_state[L, 0] = self.state[D, 2]\n",
    "            new_state[L, 2] = self.state[D, 0]\n",
    "            new_state[U, 0] = self.state[L, 0]\n",
    "            new_state[U, 2] = self.state[L, 2]\n",
    "            new_state[D, 0] = self.state[R, 0]\n",
    "            new_state[D, 2] = self.state[R, 2]\n",
    "        # Right Clockwise\n",
    "        elif action == 4:\n",
    "            new_state[F, 2] = self.state[D, 2]\n",
    "            new_state[F, 3] = self.state[D, 3]\n",
    "            new_state[B, 2] = self.state[U, 2]\n",
    "            new_state[B, 3] = self.state[U, 3]\n",
    "            new_state[R, 0] = self.state[R, 2]\n",
    "            new_state[R, 1] = self.state[R, 0]\n",
    "            new_state[R, 2] = self.state[R, 3]\n",
    "            new_state[R, 3] = self.state[R, 1]\n",
    "            new_state[U, 2] = self.state[F, 3]\n",
    "            new_state[U, 3] = self.state[F, 2]\n",
    "            new_state[D, 2] = self.state[B, 3]\n",
    "            new_state[D, 3] = self.state[B, 2]\n",
    "        # Right Counter-Clockwise\n",
    "        elif action == 5:\n",
    "            new_state[F, 2] = self.state[U, 3]\n",
    "            new_state[F, 3] = self.state[U, 2]\n",
    "            new_state[B, 2] = self.state[D, 3]\n",
    "            new_state[B, 3] = self.state[D, 2]\n",
    "            new_state[R, 0] = self.state[R, 1]\n",
    "            new_state[R, 1] = self.state[R, 3]\n",
    "            new_state[R, 2] = self.state[R, 0]\n",
    "            new_state[R, 3] = self.state[R, 2]\n",
    "            new_state[U, 2] = self.state[B, 2]\n",
    "            new_state[U, 3] = self.state[B, 3]\n",
    "            new_state[D, 2] = self.state[F, 2]\n",
    "            new_state[D, 3] = self.state[F, 3]\n",
    "        # Left Clockwise\n",
    "        elif action == 6:\n",
    "            new_state[F, 0] = self.state[U, 1]\n",
    "            new_state[F, 1] = self.state[U, 0]\n",
    "            new_state[B, 0] = self.state[D, 1]\n",
    "            new_state[B, 1] = self.state[D, 0]\n",
    "            new_state[L, 0] = self.state[L, 1]\n",
    "            new_state[L, 1] = self.state[L, 3]\n",
    "            new_state[L, 2] = self.state[L, 0]\n",
    "            new_state[L, 3] = self.state[L, 2]\n",
    "            new_state[U, 0] = self.state[B, 0]\n",
    "            new_state[U, 1] = self.state[B, 1]\n",
    "            new_state[D, 0] = self.state[F, 0]\n",
    "            new_state[D, 1] = self.state[F, 1]\n",
    "        # Left Counter-Clockwise\n",
    "        elif action == 7:\n",
    "            new_state[F, 0] = self.state[D, 0]\n",
    "            new_state[F, 1] = self.state[D, 1]\n",
    "            new_state[B, 0] = self.state[U, 0]\n",
    "            new_state[B, 1] = self.state[U, 1]\n",
    "            new_state[L, 0] = self.state[L, 2]\n",
    "            new_state[L, 1] = self.state[L, 0]\n",
    "            new_state[L, 2] = self.state[L, 3]\n",
    "            new_state[L, 3] = self.state[L, 1]\n",
    "            new_state[U, 0] = self.state[F, 1]\n",
    "            new_state[U, 1] = self.state[F, 0]\n",
    "            new_state[D, 0] = self.state[B, 1]\n",
    "            new_state[D, 1] = self.state[B, 0]\n",
    "        # Up Clockwise\n",
    "        elif action == 8:\n",
    "            new_state[F, 1] = self.state[R, 3]\n",
    "            new_state[F, 3] = self.state[R, 2]\n",
    "            new_state[B, 1] = self.state[L, 3]\n",
    "            new_state[B, 3] = self.state[L, 2]\n",
    "            new_state[R, 2] = self.state[B, 1]\n",
    "            new_state[R, 3] = self.state[B, 3]\n",
    "            new_state[L, 2] = self.state[F, 1]\n",
    "            new_state[L, 3] = self.state[F, 3]\n",
    "            new_state[U, 0] = self.state[U, 1]\n",
    "            new_state[U, 1] = self.state[U, 3]\n",
    "            new_state[U, 2] = self.state[U, 0]\n",
    "            new_state[U, 3] = self.state[U, 2]\n",
    "        # Up Counter-Clockwise\n",
    "        elif action == 9:\n",
    "            new_state[F, 1] = self.state[L, 2]\n",
    "            new_state[F, 3] = self.state[L, 3]\n",
    "            new_state[B, 1] = self.state[R, 2]\n",
    "            new_state[B, 3] = self.state[R, 3]\n",
    "            new_state[R, 2] = self.state[F, 3]\n",
    "            new_state[R, 3] = self.state[F, 1]\n",
    "            new_state[L, 2] = self.state[B, 3]\n",
    "            new_state[L, 3] = self.state[B, 1]\n",
    "            new_state[U, 0] = self.state[U, 2]\n",
    "            new_state[U, 1] = self.state[U, 0]\n",
    "            new_state[U, 2] = self.state[U, 3]\n",
    "            new_state[U, 3] = self.state[U, 1]\n",
    "        # Bottom Clockwise\n",
    "        elif action == 10:\n",
    "            new_state[F, 0] = self.state[L, 0]\n",
    "            new_state[F, 2] = self.state[L, 1]\n",
    "            new_state[B, 0] = self.state[R, 0]\n",
    "            new_state[B, 2] = self.state[R, 1]\n",
    "            new_state[R, 0] = self.state[F, 2]\n",
    "            new_state[R, 1] = self.state[F, 0]\n",
    "            new_state[L, 0] = self.state[B, 2]\n",
    "            new_state[L, 1] = self.state[B, 0]\n",
    "            new_state[D, 0] = self.state[D, 2]\n",
    "            new_state[D, 1] = self.state[D, 0]\n",
    "            new_state[D, 2] = self.state[D, 3]\n",
    "            new_state[D, 3] = self.state[D, 1]\n",
    "        # Bottom Counter-Clockwise\n",
    "        elif action == 11:\n",
    "            new_state[F, 0] = self.state[R, 1]\n",
    "            new_state[F, 2] = self.state[R, 0]\n",
    "            new_state[B, 0] = self.state[L, 1]\n",
    "            new_state[B, 2] = self.state[L, 0]\n",
    "            new_state[R, 0] = self.state[B, 0]\n",
    "            new_state[R, 1] = self.state[B, 2]\n",
    "            new_state[L, 0] = self.state[F, 0]\n",
    "            new_state[L, 1] = self.state[F, 2]\n",
    "            new_state[D, 0] = self.state[D, 1]\n",
    "            new_state[D, 1] = self.state[D, 3]\n",
    "            new_state[D, 2] = self.state[D, 0]\n",
    "            new_state[D, 3] = self.state[D, 2]\n",
    "        self.state = new_state\n",
    "        return self._get_obs(), 1 if self._is_solved() else -1, self._is_solved(), self.step_count >= 100, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        one_hots = []\n",
    "        for i in range(6):\n",
    "            for j in range(4):\n",
    "                label = int(self.state[i, j])\n",
    "                zeros = np.zeros(6)\n",
    "                zeros[label] = 1\n",
    "                one_hots.append(zeros)\n",
    "        return np.array(one_hots)\n",
    "    \n",
    "    def _is_solved(self):\n",
    "        for i in range(6):\n",
    "            if np.mean(self.state[i]) != self.state[i][0]:\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "624c83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def state(self):\n",
    "        return self.env.state\n",
    "    \n",
    "    def reset(self, *args, **kwargs):\n",
    "        super().reset(*args, **kwargs)\n",
    "        self.env.step(self.env.action_space.sample())\n",
    "        self.env.step(self.env.action_space.sample())\n",
    "        return self.env._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, _ = super().step(action)\n",
    "        if terminated:\n",
    "            reward = 100\n",
    "        return obs, reward, terminated, truncated, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f8b4d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.4     |\n",
      "|    ep_rew_mean      | -84.3    |\n",
      "|    exploration_rate | 0.132    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 4624     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 9136     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00031  |\n",
      "|    n_updates        | 2258     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.9     |\n",
      "|    ep_rew_mean      | -76.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 4407     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 17928    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00032  |\n",
      "|    n_updates        | 4456     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 80.2     |\n",
      "|    ep_rew_mean      | -61      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 4300     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 25946    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000486 |\n",
      "|    n_updates        | 6461     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71.3     |\n",
      "|    ep_rew_mean      | -43      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 4189     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 33072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000479 |\n",
      "|    n_updates        | 8242     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 62.8     |\n",
      "|    ep_rew_mean      | -23.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 4123     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 39348    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000449 |\n",
      "|    n_updates        | 9811     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 54.2     |\n",
      "|    ep_rew_mean      | -6.69    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 4072     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 44764    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000499 |\n",
      "|    n_updates        | 11165    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 37.5     |\n",
      "|    ep_rew_mean      | 27.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 4063     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 48514    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000346 |\n",
      "|    n_updates        | 12103    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 38.3     |\n",
      "|    ep_rew_mean      | 26.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 4067     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 52346    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000947 |\n",
      "|    n_updates        | 13061    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 37.5     |\n",
      "|    ep_rew_mean      | 28.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 4076     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 56094    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00122  |\n",
      "|    n_updates        | 13998    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.9     |\n",
      "|    ep_rew_mean      | 3.65     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 4092     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61082    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0014   |\n",
      "|    n_updates        | 15245    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 42.8     |\n",
      "|    ep_rew_mean      | 16.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100     |\n",
      "|    fps              | 4106     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 65360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 16314    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 30.9     |\n",
      "|    ep_rew_mean      | 40.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200     |\n",
      "|    fps              | 4113     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 68446    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00337  |\n",
      "|    n_updates        | 17086    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 35       |\n",
      "|    ep_rew_mean      | 33.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1300     |\n",
      "|    fps              | 4122     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 71948    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.039    |\n",
      "|    n_updates        | 17961    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31.7     |\n",
      "|    ep_rew_mean      | 39       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1400     |\n",
      "|    fps              | 4128     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 75122    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00402  |\n",
      "|    n_updates        | 18755    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.7     |\n",
      "|    ep_rew_mean      | 47       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1500     |\n",
      "|    fps              | 4129     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 77894    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00105  |\n",
      "|    n_updates        | 19448    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 35.5     |\n",
      "|    ep_rew_mean      | 31.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1600     |\n",
      "|    fps              | 4125     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 81440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00372  |\n",
      "|    n_updates        | 20334    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.9     |\n",
      "|    ep_rew_mean      | 46.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1700     |\n",
      "|    fps              | 4122     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 84230    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 21032    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 34.3     |\n",
      "|    ep_rew_mean      | 33.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1800     |\n",
      "|    fps              | 4122     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 87656    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00412  |\n",
      "|    n_updates        | 21888    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.3     |\n",
      "|    ep_rew_mean      | 48.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1900     |\n",
      "|    fps              | 4122     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 90384    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.7      |\n",
      "|    n_updates        | 22570    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 35.2     |\n",
      "|    ep_rew_mean      | 31.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2000     |\n",
      "|    fps              | 4106     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 93900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 23449    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.8     |\n",
      "|    ep_rew_mean      | 51       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2100     |\n",
      "|    fps              | 4105     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 96476    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0174   |\n",
      "|    n_updates        | 24093    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.7     |\n",
      "|    ep_rew_mean      | 47       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2200     |\n",
      "|    fps              | 4110     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 99250    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.27     |\n",
      "|    n_updates        | 24787    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = Cube2()\n",
    "env = RewardWrapper(env)\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=100000, log_interval=100)\n",
    "model.save(\"dqn_cube2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "24132717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotationController.setState([[4.0, 4.0, 0.0, 0.0], [1.0, 5.0, 1.0, 5.0], [4.0, 2.0, 1.0, 2.0], [5.0, 3.0, 0.0, 3.0], [3.0, 1.0, 3.0, 4.0], [2.0, 0.0, 2.0, 5.0]])\n",
      "rotationController.addRotationStepCode(...[7, 10, 2, 5, 7, 4, 10, 3, 11, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4])\n",
      "\n",
      "Solved in 98 steps\n"
     ]
    }
   ],
   "source": [
    "# model = DQN.load(\"dqn_cube2.pkl\")\n",
    "import json\n",
    "\n",
    "env = Cube2()\n",
    "env = RewardWrapper(env)\n",
    "obs, _ = env.reset()\n",
    "print(f\"rotationController.setState({json.dumps(env.state().tolist())})\")\n",
    "\n",
    "solved_actions = []\n",
    "for i in range(100):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    solved_actions.append(action.item())\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "print(f\"rotationController.addRotationStepCode(...{json.dumps(solved_actions)})\")\n",
    "\n",
    "print()\n",
    "print(f\"Solved in {len(solved_actions)} steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39924b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
